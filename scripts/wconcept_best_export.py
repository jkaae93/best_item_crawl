#!/usr/bin/env python3
from __future__ import annotations

import argparse
import csv
import json
import os
import re
import time
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import requests
from playwright.sync_api import TimeoutError as PlaywrightTimeoutError
from playwright.sync_api import sync_playwright
from zoneinfo import ZoneInfo

KST = ZoneInfo("Asia/Seoul")
BEST_PAGE_URL = (
    "https://display.wconcept.co.kr/rn/best?displayCategoryType=ALL&displaySubCategoryType=ALL&gnbType=Y"
)
# í˜ì´ì§€ê°€ ìµœì´ˆë¡œ í˜¸ì¶œí•˜ëŠ” ìƒí’ˆ APIë¥¼ ê´€ì°°í•˜ì—¬ ì¹´í…Œê³ ë¦¬ íŒíŠ¸ë¥¼ í™•ë³´í•œë‹¤
CATEGORY_ENDPOINT_SUBSTR = "/display/api/best/v1/product"
PRODUCT_ENDPOINT = "https://gw-front.wconcept.co.kr/display/api/best/v1/product"

ALLOWED_BRANDS = ["í•˜ì‹œì—", "HACIE"]


@dataclass(frozen=True)
class CategoryPair:
    depth1_code: str
    depth1_name: str
    depth2_code: str
    depth2_name: str


def find_key_recursive(obj: Any, key: str) -> List[Any]:
    results: List[Any] = []
    if isinstance(obj, dict):
        for k, v in obj.items():
            if k == key:
                results.append(v)
            results.extend(find_key_recursive(v, key))
    elif isinstance(obj, list):
        for item in obj:
            results.extend(find_key_recursive(item, key))
    return results


def iter_dicts(obj: Any) -> Iterable[Dict[str, Any]]:
    if isinstance(obj, dict):
        yield obj
        for v in obj.values():
            yield from iter_dicts(v)
    elif isinstance(obj, list):
        for item in obj:
            yield from iter_dicts(item)


def extract_category_pairs(categories_json: Dict[str, Any]) -> List[CategoryPair]:
    pairs: List[CategoryPair] = []

    # bestCategories.category1DepthList êµ¬ì¡° ì²˜ë¦¬ (Next.js __NEXT_DATA__)
    best_categories = categories_json.get("bestCategories", {})
    if best_categories and isinstance(best_categories, dict):
        category1_depth_list = best_categories.get("category1DepthList", [])
        
        if category1_depth_list and isinstance(category1_depth_list, list):
            print(f"   ğŸ¯ bestCategories ë°œê²¬: {len(category1_depth_list)}ê°œ depth1")
            
            for depth1_item in category1_depth_list:
                if not isinstance(depth1_item, dict):
                    continue
                
                d1_code = str(depth1_item.get("depth1Code", ""))
                d1_name = str(depth1_item.get("depth1Name", ""))
                
                # category2DepthListì—ì„œ depth2 ì¶”ì¶œ (nullì¼ ìˆ˜ë„ ìˆìŒ)
                category2_depth_list = depth1_item.get("category2DepthList")
                
                # category2DepthListê°€ nullì¸ ê²½ìš° (ìµœìƒìœ„ ì „ì²´)
                if category2_depth_list is None and d1_code == "ALL":
                    pairs.append(CategoryPair(d1_code, d1_name, "ALL", "ì „ì²´"))
                elif category2_depth_list and isinstance(category2_depth_list, list):
                    # ì¼ë°˜ì ì¸ ê²½ìš°: depth2 ëª©ë¡ ìˆœíšŒ
                    for depth2_item in category2_depth_list:
                        if not isinstance(depth2_item, dict):
                            continue
                        
                        d2_code = str(depth2_item.get("depth2Code", ""))
                        d2_name = str(depth2_item.get("depth2Name", ""))
                        
                        if d1_code and d2_code:
                            pairs.append(CategoryPair(d1_code, d1_name, d2_code, d2_name))
    
    # lnbInfo êµ¬ì¡° ì²˜ë¦¬ (Next.js initialData - ëŒ€ì²´ ë°©ë²•)
    if not pairs:
        lnb_info = categories_json.get("lnbInfo", [])
        if lnb_info and isinstance(lnb_info, list):
            for depth1_group in lnb_info:
                if not isinstance(depth1_group, dict):
                    continue
                
                # depth1 ì •ë³´
                d1_code = str(depth1_group.get("largeCategory") or depth1_group.get("depth1Code") or "")
                d1_name = str(depth1_group.get("mediumName") or depth1_group.get("mediumKorName") or depth1_group.get("depth1Name") or "")
                
                # categoryDetailì´ë‚˜ subCategoriesì—ì„œ depth2 ì¶”ì¶œ
                sub_categories = depth1_group.get("categoryDetail", []) or depth1_group.get("subCategories", [])
                
                for depth2_item in sub_categories:
                    if not isinstance(depth2_item, dict):
                        continue
                        
                    d2_code = str(depth2_item.get("middleCategory") or depth2_item.get("depth2Code") or "")
                    d2_name = str(depth2_item.get("categoryName") or depth2_item.get("depth2Name") or "")
                    
                    if d1_code and d2_code:
                        pairs.append(CategoryPair(d1_code, d1_name, d2_code, d2_name))
    
    # ê¸°ì¡´ ë¡œì§: bestCategories í‚¤ ì°¾ê¸° (ì¬ê·€ ê²€ìƒ‰)
    if not pairs:
        candidate_arrays = find_key_recursive(categories_json, "bestCategories")
        if not candidate_arrays:
            candidate_arrays = find_key_recursive(categories_json, "categories")

        for arr in candidate_arrays:
            if not isinstance(arr, list):
                continue
            for group in arr:
                if not isinstance(group, dict):
                    continue
                d1_code = str(
                    group.get("depth1Code")
                    or group.get("depth1Cd")
                    or group.get("d1Code")
                    or group.get("code")
                    or ""
                )
                d1_name = str(
                    group.get("depth1Name")
                    or group.get("d1Name")
                    or group.get("name")
                    or ""
                )
                if not d1_code:
                    # Try to find a depth1 code in any nested object
                    for d in iter_dicts(group):
                        cand = d.get("depth1Code") or d.get("depth1Cd") or d.get("d1Code")
                        if cand:
                            d1_code = str(cand)
                            d1_name = str(d.get("depth1Name") or d.get("d1Name") or d.get("name") or d1_name)
                            break

                # Collect all depth2 entries under this group
                for d in iter_dicts(group):
                    d2_code = d.get("depth2Code") or d.get("depth2Cd") or d.get("d2Code")
                    if not d2_code:
                        continue
                    d2_name = str(d.get("depth2Name") or d.get("d2Name") or d.get("name") or "")
                    if d1_code:
                        pairs.append(
                            CategoryPair(
                                depth1_code=str(d1_code),
                                depth1_name=d1_name,
                                depth2_code=str(d2_code),
                                depth2_name=d2_name,
                            )
                        )

    # Deduplicate by (d1, d2)
    unique: Dict[Tuple[str, str], CategoryPair] = {}
    for p in pairs:
        key = (p.depth1_code, p.depth2_code)
        if key not in unique and p.depth1_code and p.depth2_code:
            unique[key] = p
    return list(unique.values())


def get_api_key_and_categories(timeout_ms: int = 25000) -> Tuple[str, List[CategoryPair], Dict[str, str]]:
    """ë² ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ __NEXT_DATA__ë¥¼ í†µí•´ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    
    pairs: List[CategoryPair] = []
    captured_headers: Dict[str, str] = {}
    
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        context = browser.new_context(locale="ko-KR", timezone_id="Asia/Seoul")
        page = context.new_page()

        def on_response(response):
            nonlocal captured_headers
            url = response.url
            
            # ìƒí’ˆ APIì—ì„œ í—¤ë” ì¶”ì¶œ
            if CATEGORY_ENDPOINT_SUBSTR in url:
                try:
                    req = response.request
                    headers = req.headers or {}
                    captured_headers = {k.lower(): v for k, v in headers.items()}
                except Exception:
                    pass

        page.on("response", on_response)
        
        print("ğŸ” ë² ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ __NEXT_DATA__ ì¶”ì¶œ...")
        
        try:
            page.goto(BEST_PAGE_URL, wait_until="networkidle", timeout=timeout_ms)
            page.wait_for_timeout(2000)
            
            # __NEXT_DATA__ì—ì„œ bestCategories ì¶”ì¶œ
            next_data = page.evaluate("""
                () => {
                    const nextDataScript = document.getElementById('__NEXT_DATA__');
                    if (nextDataScript && nextDataScript.textContent) {
                        try {
                            const data = JSON.parse(nextDataScript.textContent);
                            const initialData = data?.props?.pageProps?.initialData;
                            
                            if (initialData && initialData.bestCategories) {
                                return initialData.bestCategories;
                            }
                        } catch (e) {
                            console.error('Parse error:', e);
                        }
                    }
                    return null;
                }
            """)
            
            if next_data and next_data.get('category1DepthList'):
                cat1_list = next_data.get('category1DepthList', [])
                print(f"âœ… bestCategories ë°œê²¬: {len(cat1_list)}ê°œ depth1 ì¹´í…Œê³ ë¦¬")
                
                # ì¹´í…Œê³ ë¦¬ íŒŒì‹±
                pairs = extract_category_pairs({"bestCategories": next_data})
                print(f"ğŸ¯ ì´ {len(pairs)}ê°œ ì¹´í…Œê³ ë¦¬ ì¡°í•© ì¶”ì¶œ")
            else:
                print("âš ï¸ bestCategoriesë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                    
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ ë¡œë“œ ì‹¤íŒ¨: {e}")

        context.close()
        browser.close()
    
    # ì‹¤íŒ¨ ì‹œ Wì»¨ì…‰ì˜ ì£¼ìš” ì¹´í…Œê³ ë¦¬ í•˜ë“œì½”ë”©
    if not pairs:
        print("âš ï¸ ì¹´í…Œê³ ë¦¬ ë™ì  ì¶”ì¶œ ì‹¤íŒ¨, í•˜ë“œì½”ë”©ëœ ì¹´í…Œê³ ë¦¬ ì‚¬ìš©")
        pairs = [
            # 1. ì˜ë¥˜ - 12ê°œ
            CategoryPair("10102", "ì˜ë¥˜", "10102101", "ì•„ìš°í„°"),
            CategoryPair("10102", "ì˜ë¥˜", "10102201", "ì›í”¼ìŠ¤"),
            CategoryPair("10102", "ì˜ë¥˜", "10102202", "ìƒì˜"),
            CategoryPair("10102", "ì˜ë¥˜", "10102203", "í•˜ì˜"),
            CategoryPair("10102", "ì˜ë¥˜", "10102204", "ì…”ì¸ /ë¸”ë¼ìš°ìŠ¤"),
            CategoryPair("10102", "ì˜ë¥˜", "10102205", "ë‹ˆíŠ¸/ìŠ¤ì›¨í„°"),
            CategoryPair("10102", "ì˜ë¥˜", "10102206", "ì„¸íŠ¸"),
            CategoryPair("10102", "ì˜ë¥˜", "10102207", "ìŠ¤ì»¤íŠ¸"),
            CategoryPair("10102", "ì˜ë¥˜", "10102208", "í‹°ì…”ì¸ "),
            CategoryPair("10102", "ì˜ë¥˜", "10102209", "íŒ¬ì¸ "),
            CategoryPair("10102", "ì˜ë¥˜", "10102210", "ì í”„ìˆ˜íŠ¸"),
            CategoryPair("10102", "ì˜ë¥˜", "10102211", "ë°ë‹˜"),
            # 2. ìŠˆì¦ˆ - 6ê°œ
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103101", "ìŠ¤ë‹ˆì»¤ì¦ˆ"),
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103102", "í”Œë«/ë¡œí¼"),
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103103", "ìƒŒë“¤/ìŠ¬ë¦¬í¼"),
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103104", "í/íŒí”„ìŠ¤"),
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103105", "ë¶€ì¸ /ì›Œì»¤"),
            CategoryPair("10103", "ìŠˆì¦ˆ", "10103106", "ìŠ¬ë¦½ì˜¨"),
            # 3. ê°€ë°© - 7ê°œ
            CategoryPair("10104", "ê°€ë°©", "10104101", "ìˆ„ë”ë°±"),
            CategoryPair("10104", "ê°€ë°©", "10104102", "í¬ë¡œìŠ¤ë°±"),
            CategoryPair("10104", "ê°€ë°©", "10104103", "í† íŠ¸ë°±"),
            CategoryPair("10104", "ê°€ë°©", "10104104", "í´ëŸ¬ì¹˜"),
            CategoryPair("10104", "ê°€ë°©", "10104105", "ë°±íŒ©"),
            CategoryPair("10104", "ê°€ë°©", "10104106", "ì—ì½”ë°±"),
            CategoryPair("10104", "ê°€ë°©", "10104107", "ìºë¦¬ì–´"),
            # 4. ì•¡ì„¸ì„œë¦¬ - 8ê°œ
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105101", "ì£¼ì–¼ë¦¬"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105102", "ì‹œê³„"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105103", "ëª¨ì"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105104", "ë²¨íŠ¸"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105105", "ì–‘ë§"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105106", "í—¤ì–´"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105107", "ì„ ê¸€ë¼ìŠ¤"),
            CategoryPair("10105", "ì•¡ì„¸ì„œë¦¬", "10105108", "ìŠ¤ì¹´í”„"),
            # 5. ë·°í‹° - 6ê°œ
            CategoryPair("10106", "ë·°í‹°", "10106101", "ìŠ¤í‚¨ì¼€ì–´"),
            CategoryPair("10106", "ë·°í‹°", "10106102", "ë©”ì´í¬ì—…"),
            CategoryPair("10106", "ë·°í‹°", "10106103", "ë°”ë””ì¼€ì–´"),
            CategoryPair("10106", "ë·°í‹°", "10106104", "í—¤ì–´ì¼€ì–´"),
            CategoryPair("10106", "ë·°í‹°", "10106105", "í–¥ìˆ˜"),
            CategoryPair("10106", "ë·°í‹°", "10106106", "ë„¤ì¼"),
            # 6. ë¼ì´í”„ - 4ê°œ
            CategoryPair("10107", "ë¼ì´í”„", "10107101", "ë¦¬ë¹™"),
            CategoryPair("10107", "ë¼ì´í”„", "10107102", "í…Œí¬"),
            CategoryPair("10107", "ë¼ì´í”„", "10107103", "ì‹í’ˆ"),
            CategoryPair("10107", "ë¼ì´í”„", "10107104", "ë¬¸êµ¬"),
            # 7. ë§¨ì¦ˆ - 6ê°œ
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108101", "ì˜ë¥˜"),
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108102", "ìŠˆì¦ˆ"),
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108103", "ê°€ë°©"),
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108104", "ì•¡ì„¸ì„œë¦¬"),
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108105", "ë·°í‹°"),
            CategoryPair("10108", "ë§¨ì¦ˆ", "10108106", "ìŠ¤í¬ì¸ "),
            # 8. í‚¤ì¦ˆ - 4ê°œ
            CategoryPair("10109", "í‚¤ì¦ˆ", "10109101", "ì˜ë¥˜"),
            CategoryPair("10109", "í‚¤ì¦ˆ", "10109102", "ìŠˆì¦ˆ"),
            CategoryPair("10109", "í‚¤ì¦ˆ", "10109103", "ê°€ë°©"),
            CategoryPair("10109", "í‚¤ì¦ˆ", "10109104", "ì•¡ì„¸ì„œë¦¬"),
            # 9. ìŠ¤í¬ì¸  - 5ê°œ
            CategoryPair("10110", "ìŠ¤í¬ì¸ ", "10110101", "ì˜ë¥˜"),
            CategoryPair("10110", "ìŠ¤í¬ì¸ ", "10110102", "ìŠˆì¦ˆ"),
            CategoryPair("10110", "ìŠ¤í¬ì¸ ", "10110103", "ê°€ë°©"),
            CategoryPair("10110", "ìŠ¤í¬ì¸ ", "10110104", "ì•¡ì„¸ì„œë¦¬"),
            CategoryPair("10110", "ìŠ¤í¬ì¸ ", "10110105", "ìš©í’ˆ"),
            # 10. ì–¸ë”ì›¨ì–´ - 3ê°œ
            CategoryPair("10111", "ì–¸ë”ì›¨ì–´", "10111101", "ì—¬ì„±"),
            CategoryPair("10111", "ì–¸ë”ì›¨ì–´", "10111102", "ë‚¨ì„±"),
            CategoryPair("10111", "ì–¸ë”ì›¨ì–´", "10111103", "í™ˆì›¨ì–´"),
        ]
        print(f"ğŸ“‹ í•˜ë“œì½”ë”©ëœ ì¹´í…Œê³ ë¦¬ {len(pairs)}ê°œ ì‚¬ìš© (depth1: 10ê°œ)")

    # Prepare base headers for subsequent API calls
    base_headers = {
        "content-type": "application/json",
        "origin": "https://display.wconcept.co.kr",
        "referer": "https://display.wconcept.co.kr/rn/best",
        "user-agent": captured_headers.get("user-agent", "Mozilla/5.0"),
    }
    
    return None, pairs, base_headers


def extract_products_list(obj: Any) -> List[Dict[str, Any]]:
    # Try common keys first
    for key in ("products", "productList", "list", "items", "bestProducts"):
        vals = find_key_recursive(obj, key)
        for v in vals:
            if isinstance(v, list) and v and isinstance(v[0], dict):
                return v
    # Common Wconcept best API shape: { data: { content: [...] } }
    if isinstance(obj, dict):
        data = obj.get("data")
        if isinstance(data, dict):
            content = data.get("content")
            if isinstance(content, list) and content and isinstance(content[0], dict):
                return content
    # Fallback: return any list of dicts containing 'productName' like fields
    candidates = find_key_recursive(obj, "productName")
    if candidates:
        # If 'productName' values exist, try to traverse back to lists (not trivial here); fallback below
        pass
    # As safest fallback, scan for any list of dicts with expected keys
    def has_product_shape(d: Dict[str, Any]) -> bool:
        keys = set(k.lower() for k in d.keys())
        return any(k in keys for k in ("productname", "name", "brandname"))

    lists: List[List[Dict[str, Any]]] = []
    for v in find_key_recursive(obj, "data") + find_key_recursive(obj, "result") + [obj]:
        if isinstance(v, list) and v and isinstance(v[0], dict) and has_product_shape(v[0]):
            lists.append(v)
    if lists:
        return lists[0]
    return []


def pick_price(product: Dict[str, Any]) -> Optional[int]:
    # Try common price fields
    for key in ("salePrice", "finalPrice", "price", "discountPrice", "sale_price"):
        if key in product and isinstance(product[key], (int, float, str)):
            try:
                return int(float(str(product[key]).replace(",", "")))
            except Exception:
                continue
    return None


def pick_name(product: Dict[str, Any]) -> str:
    for key in ("productName", "name", "goodsName", "title"):
        if key in product and product[key]:
            return str(product[key])
    return ""


def pick_brand(product: Dict[str, Any]) -> str:
    for key in ("brandName", "brand", "brand_name"):
        if key in product and product[key]:
            return str(product[key])
    return ""


def pick_rank(idx: int, product: Dict[str, Any]) -> int:
    for key in ("rank", "ranking", "bestOrder", "exposeOrder", "order"):
        if key in product:
            try:
                return int(product[key])
            except Exception:
                continue
    return idx + 1


def filter_products_by_brand(products: List[Dict[str, Any]], allowed_brands: List[str]) -> List[Dict[str, Any]]:
    if not products:
        return []
    allowed_exact_korean = {b.strip() for b in allowed_brands if b.strip() and not b.strip().isascii()}
    allowed_english_casefold = {b.strip().casefold() for b in allowed_brands if b.strip() and b.strip().isascii()}

    filtered: List[Dict[str, Any]] = []
    for p in products:
        brand = pick_brand(p).strip()
        if not brand:
            continue
        if brand in allowed_exact_korean:
            filtered.append(p)
            continue
        if brand.casefold() in allowed_english_casefold:
            filtered.append(p)
    return filtered


def fetch_products_for_category_page(
    headers: Dict[str, str],
    cat: CategoryPair,
    page_no: int,
    page_size: int,
    max_retries: int = 3,
) -> Tuple[List[Dict[str, Any]], Any]:
    payload = {
        "custNo": "0",
        "domain": "WOMEN",
        "genderType": "all",
        "dateType": "daily",
        "ageGroup": "all",
        "depth1Code": cat.depth1_code if cat.depth1_code != "ALL" else "",
        "depth2Code": cat.depth2_code if cat.depth2_code != "ALL" else "",
        "pageSize": page_size,
        "pageNo": page_no,
    }
    
    last_error = None
    for attempt in range(max_retries):
        try:
            resp = requests.post(PRODUCT_ENDPOINT, headers=headers, json=payload, timeout=30)
            resp.raise_for_status()
            data = resp.json()
            products = extract_products_list(data)
            return products, data
        except requests.exceptions.HTTPError as e:
            last_error = e
            if e.response.status_code >= 500 and attempt < max_retries - 1:
                wait_time = (attempt + 1) * 2
                time.sleep(wait_time)
                continue
            raise
        except Exception as e:
            last_error = e
            if attempt < max_retries - 1:
                time.sleep(2)
                continue
            raise
    
    raise last_error if last_error else Exception("Unknown error")


def _infer_has_next_page(
    response_json: Any, current_page_no: int, page_size: int, last_page_count: int
) -> bool:
    # Try to infer pagination from common fields present in the response body
    def _dig(obj: Any) -> List[Dict[str, Any]]:
        dicts: List[Dict[str, Any]] = []
        if isinstance(obj, dict):
            dicts.append(obj)
            for v in obj.values():
                dicts.extend(_dig(v))
        elif isinstance(obj, list):
            for item in obj:
                dicts.extend(_dig(item))
        return dicts

    for d in _dig(response_json):
        # Explicit boolean flags
        for key in ("hasNext", "hasNextPage", "hasMore", "next"):
            val = d.get(key)
            if isinstance(val, bool):
                return val

        # Total pages
        total_pages = d.get("totalPages") or d.get("lastPage") or d.get("pages")
        if isinstance(total_pages, int) and total_pages > 0:
            return current_page_no < total_pages

        # Total count
        total_count = d.get("totalCount") or d.get("totalElements") or d.get("count")
        if isinstance(total_count, int) and total_count >= 0:
            return (current_page_no * page_size) < total_count

    # Fallback: if we received a full page, assume there might be a next page
    return last_page_count >= page_size


def fetch_all_products_for_category(
    headers: Dict[str, str],
    cat: CategoryPair,
    page_size: int = 200,
    max_pages: int = 0,
) -> List[Dict[str, Any]]:
    collected: List[Dict[str, Any]] = []
    current_page = 1
    while True:
        products, res_json = fetch_products_for_category_page(
            headers, cat, page_no=current_page, page_size=page_size
        )
        if not products:
            break
        collected.extend(products)

        # Stop if we've reached the configured page limit (0 means unlimited)
        if max_pages and current_page >= max_pages:
            break

        has_next = _infer_has_next_page(
            response_json=res_json,
            current_page_no=current_page,
            page_size=page_size,
            last_page_count=len(products),
        )
        if not has_next:
            break
        current_page += 1

    return collected


def write_csv(rows: List[List[Any]], output_dir: Path) -> Path:
    output_dir.mkdir(parents=True, exist_ok=True)
    now = datetime.now(KST)
    filename = f"wconcept_best_{now.strftime('%Y%m%d_%H%M')}_KST.csv"
    out_path = output_dir / filename
    headers = ["ë‚ ì§œ", "ì‹œê°„", "depth1_ì¹´í…Œê³ ë¦¬", "depth2_ì¹´í…Œê³ ë¦¬", "ìˆœìœ„", "ìƒí’ˆëª…", "ê°€ê²©"]
    with out_path.open("w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(headers)
        writer.writerows(rows)
    return out_path


def main():
    parser = argparse.ArgumentParser(description="Export Wconcept best products filtered by keyword to CSV")
    parser.add_argument("--output-dir", default="output", help="CSV ì¶œë ¥ ë””ë ‰í„°ë¦¬")
    parser.add_argument("--page-size", type=int, default=200, help="í˜ì´ì§€ë‹¹ ìƒí’ˆ ìˆ˜ (ê¸°ë³¸ 200)")
    parser.add_argument(
        "--max-pages",
        type=int,
        default=0,
        help="ìµœëŒ€ í˜ì´ì§€ ìˆ˜ (0=ì œí•œ ì—†ìŒ, ìë™ ì¢…ë£Œ)",
    )
    args = parser.parse_args()

    output_dir = Path(args.output_dir)

    try:
        api_key, categories, base_headers = get_api_key_and_categories()
    except Exception as e:
        print(f"âŒ ì¹´í…Œê³ ë¦¬ ë° API í‚¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

    kst_now = datetime.now(KST)
    date_str = kst_now.strftime("%Y-%m-%d")
    time_str = kst_now.strftime("%H:%M")

    rows: List[List[Any]] = []

    page_size = max(1, int(args.page_size))
    max_pages = max(0, int(args.max_pages))

    # bestCategoriesì— ì´ë¯¸ ALL > ì „ì²´, depth1 > ALLì´ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆìŒ
    # ì¶”ê°€ ì‘ì—… ì—†ì´ ë°”ë¡œ ì‚¬ìš©
    print(f"ğŸ” ì´ {len(categories)}ê°œ ì¹´í…Œê³ ë¦¬ ì¡°í•© ìˆ˜ì§‘ ì‹œì‘...")
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ HACIE ì œí’ˆ ìˆ˜ì§‘
    hacie_found_per_category = {}
    
    for cat in categories:
        try:
            print(f"  ğŸ“‚ {cat.depth1_name or cat.depth1_code} > {cat.depth2_name or cat.depth2_code}")
            products = fetch_all_products_for_category(
                base_headers, cat, page_size=page_size, max_pages=max_pages
            )
        except Exception as e:
            print(f"     âŒ ì˜¤ë¥˜: {e}")
            continue
        
        filtered = filter_products_by_brand(products, ALLOWED_BRANDS)
        
        # ì¹´í…Œê³ ë¦¬ë³„ HACIE ë°œê²¬ ì¹´ìš´íŠ¸
        cat_key = f"{cat.depth1_name or cat.depth1_code} > {cat.depth2_name or cat.depth2_code}"
        hacie_found_per_category[cat_key] = len(filtered)
        
        if filtered:
            print(f"     âœ… HACIE {len(filtered)}ê°œ ë°œê²¬")
        
        for idx, p in enumerate(filtered):
            rank = pick_rank(idx, p)
            name = pick_name(p)
            price = pick_price(p)
            rows.append(
                [
                    date_str,
                    time_str,
                    cat.depth1_name or cat.depth1_code,
                    cat.depth2_name or cat.depth2_code,
                    rank,
                    name,
                    price if price is not None else "",
                ]
            )
    
    # ì¹´í…Œê³ ë¦¬ë³„ HACIE ë°œê²¬ í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ HACIE ì œí’ˆ ë°œê²¬ í˜„í™©:")
    for cat_key, count in sorted(hacie_found_per_category.items(), key=lambda x: -x[1]):
        if count > 0:
            print(f"  {cat_key}: {count}ê°œ")

    if not rows:
        # Write empty CSV with headers for traceability
        out = write_csv([], output_dir)
        print(f"âœ… CSV ìƒì„± ì™„ë£Œ (ë°ì´í„° ì—†ìŒ): {out}")
        return

    out = write_csv(rows, output_dir)
    print(f"âœ… CSV ìƒì„± ì™„ë£Œ: {out}")
    print(f"ğŸ“Š ì´ {len(rows)}ê°œ ìƒí’ˆ ìˆ˜ì§‘ë¨")


if __name__ == "__main__":
    main()
